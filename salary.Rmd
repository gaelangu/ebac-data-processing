---
title: "Data Processing Assignment"
author: "Gaelan Gu, Sunil Prakash, Wang Ruoshi, Yu Yue"
output: 
  md_document:
    variant: markdown_github
---
### Introduction
We will be preparing the salary dataset, extracted from the 1994 US Census, for a logistic regression. 

We will determine whether a person makes over 50k a year; *class* will be the dependent variable.

### Data Import
```{r}
train = read.csv('salary-train.csv')
test = read.csv('salary-test.csv')
str(train)
str(test)
```

We first import our datasets and determine which columns contain missing values.

From a glance, we can tell that the *workclass*, *occupation* and *native.country* columns contain missing values, indicated by question marks.

### Setting Entries with Question Marks as NA Values
```{r}
# train set
train$workclass = as.factor(gsub('?', NA, train$workclass, fixed = T))
train$native.country = as.factor(gsub('?', NA, train$native.country, fixed = T))
train$occupation = as.factor(gsub('?', NA, train$occupation, fixed = T))

# test set
test$workclass = as.factor(gsub('?', NA, test$workclass, fixed = T))
test$native.country = as.factor(gsub('?', NA, test$native.country, fixed = T))
test$occupation = as.factor(gsub('?', NA, test$occupation, fixed = T))
```

Since the missing values exist in both the training and testing datasets, therefore we have to indicate them as NA values before we may exclude them.

### Removing Incomplete Cases
```{r}
train = train[complete.cases(train), ]
test = test[complete.cases(test), ]
str(train)
str(test)
```

We run the *complete.cases* function to remove the NA values from both datasets. After that we use the *str* function again to ascertain that the variables are in the formats we need, without anymore missing entries.

### Logistic Classifier
```{r}
fit = suppressWarnings(glm(formula = class ~ .,
          family = binomial,
          data = train))
```

We start to train our training set using a logistic classifier, with *class* as our target variable. We use the rest of the variables as input.

### Prediction the Testing Set Results
```{r}
prob_pred = predict(fit, type = 'response', newdata = test[-14])
y_pred = ifelse(prob_pred > 0.5, '>50K', '<=50K')
```

With our classifier, we predict a result using our test set.

### Confusion Matrix
```{r}
cm = table(test[, 14], y_pred)
cm
```

Model has a **15.24%** error rate.

Let us see if we can improve the error rate through feature selection.

```{r}
summary(fit)
```

We will try dropping the *race* variable as it does not appear to be significant from the p-values (mostly > 0.05).

### New Model 1
```{r}
fit_1 = suppressWarnings(glm(formula = class ~ . - race,
          family = binomial,
          data = train))

summary(fit_1)
```

### Confusion Matrix for New Model 1
```{r}
prob_pred_1 = predict(fit_1, type = 'response', newdata = test[-14])
y_pred_1 = ifelse(prob_pred_1 > 0.5, '>50K', '<=50K')

cm_1 = table(test[, 14], y_pred_1)
cm_1
```

New model has a **15.23%** error rate, which is only slightly improved.

### New Model 2
```{r}
fit_2 = suppressWarnings(glm(formula = class ~ . - race - relationship,
          family = binomial,
          data = train))

summary(fit_2)
```

We remove the *relationship* variable as well as it appears to be a less significant variable.

### Confusion Matrix for New Model 2
```{r}
prob_pred_2 = predict(fit_2, type = 'response', newdata = test[-14])
y_pred_2 = ifelse(prob_pred_2 > 0.5, '>50K', '<=50K')

cm_2 = table(test[, 14], y_pred_2)
cm_2
```

However, error rate has risen to **15.34%**, so we can stick to New Model 1 with the *race* variable removed.